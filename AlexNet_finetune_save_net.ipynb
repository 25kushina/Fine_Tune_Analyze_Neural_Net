{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from numpy import *\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pylab import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from random import sample\n",
    "from numpy import random\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from caffe_classes import class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes = 3 # my classes (1000 # ImageNet)\n",
    "train_x = zeros((1, 227,227,3)).astype(float32)\n",
    "train_y = zeros((1, n_classes))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "learning_rate = 0.01\n",
    "#net_data = load(\"bvlc_alexnet.npy\").item()\n",
    "net_data = load(\"bvlc_alexnet.npy\").item()\n",
    "new_net = net_data\n",
    "batch_size = 20\n",
    "training_iters = 252\n",
    "display_step = 1\n",
    "dropout = 0.9 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read a few images for sanity check\n",
    "\n",
    "im1 = (imread(\"dog_2002.png\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "\n",
    "im2 = (imread(\"cat_8.jpg\")[:,:,:3]).astype(float32)\n",
    "im2 = im2 - mean(im2)\n",
    "\n",
    "im3 = (imread(\"dog_31.jpg\")[:,:,:3]).astype(float32)\n",
    "im3 = im3 - mean(im3)\n",
    "\n",
    "im4 = (imread(\"flower_33.jpg\")[:,:,:3]).astype(float32)\n",
    "im4 = im4 - mean(im4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (self.feed('data')\n",
    "#         .conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm1')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "#         .conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm2')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "#         .conv(3, 3, 384, 1, 1, name='conv3')\n",
    "#         .conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "#         .conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "#         .fc(4096, name='fc6')\n",
    "#         .fc(4096, name='fc7')\n",
    "#         .fc(1000, relu=False, name='fc8') --> change to 3 outputs\n",
    "#         .softmax(name='prob'))\n",
    "\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)   \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups = tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(3, group, kernel)\n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    \n",
    "    #conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "    k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "    conv1_in = conv(x, weights['conv1W'], biases['conv1b'], k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "    conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "    #lrn(2, 2e-05, 0.75, name='norm1')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "\n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    #conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "    k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv2_in = conv(maxpool1, weights['conv2W'], biases['conv2b'], k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv2 = tf.nn.relu(conv2_in)\n",
    "    \n",
    "    #lrn(2, 2e-05, 0.75, name='norm2')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2, depth_radius=radius, alpha=alpha, beta=beta, bias=bias)\n",
    "    \n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    #conv(3, 3, 384, 1, 1, name='conv3')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "    conv3_in = conv(maxpool2, weights['conv3W'], biases['conv3b'], k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv3 = tf.nn.relu(conv3_in)\n",
    "    \n",
    "    #conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "    conv4_in = conv(conv3, weights['conv4W'], biases['conv4b'], k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv4 = tf.nn.relu(conv4_in)\n",
    "    \n",
    "    #conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "    k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv5_in = conv(conv4, weights['conv5W'], biases['conv5b'], k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv5 = tf.nn.relu(conv5_in)\n",
    "    \n",
    "    #max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    #fc(4096, name='fc6')\n",
    "    fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), weights['fc6W'], biases['fc6B'])\n",
    "    fc6 = tf.nn.dropout(fc6, dropout)\n",
    "    \n",
    "    #fc(4096, name='fc7')\n",
    "    fc7 = tf.nn.relu_layer(fc6, weights['fc7W'], biases['fc7B'])\n",
    "    fc7 = tf.nn.dropout(fc7, dropout)\n",
    "    \n",
    "    #fc(1000, relu=False, name='fc8')\n",
    "    fc8 = tf.nn.xw_plus_b(fc7, weights['fc8W'], biases['fc8B'])\n",
    "    \n",
    "    #softmax(name='prob'))\n",
    "    #!prob = tf.nn.softmax(fc8)\n",
    "    \n",
    "    #return prob\n",
    "    return fc8\n",
    "\n",
    "def getfc8w(weights):\n",
    "    return weights['fc8W']\n",
    "def getfc7w(weights):\n",
    "    return weights['fc7W']\n",
    "def getfc6w(weights):\n",
    "    return weights['fc6W']\n",
    "def getfc8b(biases):\n",
    "    return biases['fc8B']\n",
    "def getfc7b(biases):\n",
    "    return biases['fc7B']\n",
    "def getfc6b(biases):\n",
    "    return biases['fc6B']\n",
    "\n",
    "\n",
    "def getFeat(x):\n",
    "    return prob(x,weights,biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {    \n",
    "    'conv1W': tf.Variable(net_data[\"conv1\"][0], name=\"c1w\"),\n",
    "    'conv2W': tf.Variable(net_data[\"conv2\"][0], name=\"c2w\"),\n",
    "    'conv3W': tf.Variable(net_data[\"conv3\"][0], name=\"c3w\"),\n",
    "    'conv4W': tf.Variable(net_data[\"conv4\"][0], name=\"c4w\"),\n",
    "    'conv5W': tf.Variable(net_data[\"conv5\"][0], name=\"c5w\"),\n",
    "    'fc6W': tf.Variable(net_data[\"fc6\"][0], name=\"fc6w\"),\n",
    "    'fc7W': tf.Variable(net_data[\"fc7\"][0], name=\"fc7w\"),\n",
    "    'fc8W': tf.Variable(tf.random_normal([4096, n_classes]),trainable=True, name=\"fc8w\")\n",
    "}\n",
    "\n",
    "biases = {    \n",
    "    'conv1b': tf.Variable(net_data[\"conv1\"][1], name=\"c1b\"),\n",
    "    'conv2b': tf.Variable(net_data[\"conv2\"][1], name=\"c2b\"),\n",
    "    'conv3b': tf.Variable(net_data[\"conv3\"][1], name=\"c3b\"),\n",
    "    'conv4b': tf.Variable(net_data[\"conv4\"][1], name=\"c4b\"),\n",
    "    'conv5b': tf.Variable(net_data[\"conv5\"][1], name=\"c5b\"),\n",
    "    'fc6B': tf.Variable(net_data[\"fc6\"][1], name=\"fc6b\"),\n",
    "    'fc7B': tf.Variable(net_data[\"fc7\"][1], name=\"fc7b\"),\n",
    "    'fc8B': tf.Variable(tf.random_normal([n_classes]),trainable=True, name=\"fc8b\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None,) + xdim) # None = number of input images\n",
    "y = tf.placeholder(tf.float32, [None,ydim])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "# Construct model\n",
    "prob = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# analysis\n",
    "currentfc8w = getfc8w(weights)\n",
    "currentfc7w = getfc7w(weights)\n",
    "currentfc6w = getfc6w(weights)\n",
    "currentfc8b = getfc8b(biases)\n",
    "currentfc7b = getfc7b(biases)\n",
    "currentfc6b = getfc6b(biases)\n",
    "\n",
    "feat = tf.nn.softmax(prob)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prob, y))\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "opt_vars = [v for v in tf.trainable_variables() if v.name.startswith(\"fc8\")]\n",
    "#opt_vars = [v for v in tf.trainable_variables() if v.name.startswith(\"fc\")]\n",
    "#print shape(opt_vars)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss, var_list = opt_vars)\n",
    "\n",
    "# Evaluation\n",
    "correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size):\n",
    "    ImgPath = \"/ImgDir/\"\n",
    "    files = [f for f in listdir(ImgPath) if isfile(join(ImgPath, f))]\n",
    "    files = sample(files, len(files))\n",
    "    batch_x = np.ndarray([batch_size,227, 227, 3])\n",
    "    batch_y = np.zeros((batch_size, 3))\n",
    "    i = 0\n",
    "    for fname in files:\n",
    "        img = (imread(join(ImgPath, fname))[:,:,:3]).astype(float32)\n",
    "        img = img - mean(img)\n",
    "        #img = np.reshape(img,(1,227*227*3))        \n",
    "        batch_x[i] = img\n",
    "        if \"cat\" in fname:\n",
    "            batch_y[i][0] = 1\n",
    "        if \"dog\" in fname:\n",
    "            batch_y[i][1] = 1\n",
    "        if \"flower\" in fname:\n",
    "            batch_y[i][2] = 1\n",
    "        i+=1\n",
    "        if i == batch_size:\n",
    "            yield (batch_x, batch_y)\n",
    "            batch_x = np.ndarray([batch_size,227, 227, 3])\n",
    "            batch_y = np.zeros((batch_size, 3))\n",
    "            i=0            \n",
    "#     if len(batch_x) > 0:\n",
    "#         yield (batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 20, Minibatch Loss= 80.041885, Training Accuracy= 0.30000\n",
      "Iter 40, Minibatch Loss= 60.920509, Training Accuracy= 0.45000\n",
      "Iter 60, Minibatch Loss= 68.393311, Training Accuracy= 0.30000\n",
      "Iter 80, Minibatch Loss= 63.801270, Training Accuracy= 0.40000\n",
      "Iter 100, Minibatch Loss= 42.111069, Training Accuracy= 0.60000\n",
      "Iter 120, Minibatch Loss= 16.665831, Training Accuracy= 0.70000\n",
      "Iter 140, Minibatch Loss= 11.684101, Training Accuracy= 0.75000\n",
      "Iter 160, Minibatch Loss= 13.826100, Training Accuracy= 0.70000\n",
      "Iter 180, Minibatch Loss= 7.467756, Training Accuracy= 0.80000\n",
      "Iter 200, Minibatch Loss= 1.694727, Training Accuracy= 0.95000\n",
      "Iter 220, Minibatch Loss= 7.335664, Training Accuracy= 0.85000\n",
      "Iter 240, Minibatch Loss= 9.339417, Training Accuracy= 0.85000\n",
      "Optimization Finished!\n",
      "[1 0 1 2]\n",
      "[1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Fine tune network\n",
    "init = tf.initialize_all_variables()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1    \n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y = next_batch(batch_size).next()        \n",
    "        # Run optimization op (backprop)        \n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})                                \n",
    "                \n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            cost, acc = sess.run([loss, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)                          \n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "    \n",
    "    output = sess.run(feat, feed_dict = {x:[im1,im2,im3,im4], keep_prob: 1.})\n",
    "    output2 = sess.run(prob, feed_dict = {x:[im1,im2,im3,im4], keep_prob: 1.})\n",
    "    print np.argmax(output,1)\n",
    "    print np.argmax(output2,1)\n",
    "    \n",
    "    new_net[\"fc6\"][0] = sess.run(currentfc6w)\n",
    "    new_net[\"fc6\"][1] = sess.run(currentfc6b)\n",
    "    new_net[\"fc7\"][0] = sess.run(currentfc7w)\n",
    "    new_net[\"fc7\"][1] = sess.run(currentfc7b)\n",
    "    new_net[\"fc8\"][0] = sess.run(currentfc8w)\n",
    "    new_net[\"fc8\"][1] = sess.run(currentfc8b)\n",
    "\n",
    "np.save(\"fine_tuned_net.npy\", new_net)\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    #print \"Testing Accuracy:\", \\\n",
    "    #   sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n",
    "    #                                y: mnist.test.labels[:256],\n",
    "    #                                keep_prob: 1.})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]]\n",
      "[[ -34.99688339   85.19901276 -222.18301392]\n",
      " [ 134.80635071 -186.3434906  -173.7820282 ]\n",
      " [-123.13925171  107.79956818  -89.42668915]\n",
      " [-105.69404602  -75.42551422   44.12779617]]\n",
      "[1 0 1 2]\n",
      "[1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print output\n",
    "print output2\n",
    "print np.argmax(output,1)\n",
    "print np.argmax(output2,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
